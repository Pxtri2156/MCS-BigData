{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37349,"status":"ok","timestamp":1684072130798,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"VZDUFXeI6Zv2","outputId":"5dcbe978-08ce-495a-cace-704e6dc21547"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n","E: Unable to lock directory /var/lib/apt/lists/\n","W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n","W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n","E: Could not open lock file /var/cache/apt/archives/lock - open (13: Permission denied)\n","E: Unable to lock directory /var/cache/apt/archives/\n"]}],"source":["!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","import os\n","os.environ[\"JAVA_HOME\"] = \" /usr/lib/jvm/java-11-openjdk-amd64/\"\n","os.environ[\"SPARK_HOME\"] = \"/workspace/tripx/MCS/big_data/spark-3.1.1-bin-hadoop3.2\"\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \" /usr/lib/jvm/java-11-openjdk-amd64/\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (2455456695.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    sudo add-apt-repository ppa:webupd8team/java\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["sudo add-apt-repository ppa:webupd8team/java\n","sudo apt-get update\n","sudo apt-get install oracle-java8-installer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /home/tripx/.local/lib/python3.8/site-packages (3.4.0)\n","Requirement already satisfied: py4j==0.10.9.7 in /home/tripx/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":5732,"status":"ok","timestamp":1684072136489,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"C0wrfsknnDHR","outputId":"6f75b3e0-f32e-46cf-f170-5438a23f2c94"},"outputs":[{"name":"stdout","output_type":"stream","text":["sc have not yet created!\n"]},{"ename":"RuntimeError","evalue":"Java gateway process exited before sending its port number","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c7719aa6c261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sc have not yet created!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"First app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Check spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"]}],"source":["from pyspark import SparkContext\n","# Stop spark if it existed.\n","try:\n","    sc.stop()\n","except:\n","    print('sc have not yet created!')\n","    \n","sc = SparkContext(master = \"local\", appName = \"First app\")\n","# Check spark context\n","print(sc)\n","# Check spark context version\n","print(sc.version)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"W1kOLvLxnDHT"},"source":["Lưu ý chúng ta chỉ có thể khởi tạo SparkContext 1 lần nên nếu chưa stop `sc` mà chạy lại lệnh trên sẽ bị lỗi. Do đó lệnh SparkContext.getOrCreate() được sử dụng để khởi tạo mới SparkContext nếu nó chưa xuất hiện và lấy lại SparkContext cũ nếu đã được khởi tạo và đang run."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":61,"status":"ok","timestamp":1684072136494,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"dnOOg2CUnDHU","outputId":"9423a992-a6cc-4152-b541-4e4e78ad5cc4"},"outputs":[],"source":["sc = SparkContext.getOrCreate()\n","print(sc)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"uO0K09UMnDHU"},"source":["## 2.2. Khởi tạo một Session trong SparkContext: \n","\n","Như vậy sau bước trên ta đã có môi trường kết nối tới cluster. Tuy nhiên để hoạt động được trong môi trường này thì chúng ta cần phải khởi tạo session thông qua hàm SparkSession."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":439,"status":"ok","timestamp":1684072136888,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"sZ88wRTJnDHU","outputId":"0cd455b4-8c38-4e17-83b8-9f0c7c222503"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","my_spark = SparkSession.builder.getOrCreate()\n","# Print my_spark session\n","print(my_spark)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"UePSGsGqnDHV"},"source":["Hàm getOrCreate() của session cũng tương tự như context để tránh trường hợp phát sinh lỗi khi session đã tồn tại và đang hoạt động nhưng được khởi tạo lại.\n","\n","Thuộc tính `catalog` trong 1 Session sẽ liệt kê toàn bộ các dữ liệu có bên trong 1 cluster. Trong đó để xem toàn bộ các bảng đang có trong cluster chúng ta sử dụng hàm `.listTables()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":5215,"status":"ok","timestamp":1684072142087,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"1asY1ZRlnDHV","outputId":"d3ee958a-6e6d-46a8-82bf-797f29bd55bd"},"outputs":[],"source":["# List all table exist in spark sesion\n","my_spark.catalog.listTables()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68318,"status":"ok","timestamp":1684072210383,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"3nqy7Zs0ofzM","outputId":"0f3a5e90-e8cf-4c14-e18f-4cac39419af2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"JbEogsHmnDHV"},"source":["Hiện tại trên cluster đang chưa có bảng nào. Để thêm một bảng vào cluster chúng ta có thể đọc từ my_spark.read.csv()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1684072210384,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"7D93a5Hu0HtY"},"outputs":[],"source":["flight_data_path=\"/content/drive/MyDrive/MCS/Big Data/Lab 3/Flight Data/flights.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":5418,"status":"ok","timestamp":1684072215787,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"sjwxYUKknDHW","outputId":"a47322dd-99bb-4bf4-d8d6-619e54b0a418"},"outputs":[],"source":["flights = my_spark.read.csv(flight_data_path, header = True)\n","# show fligths top 5\n","flights.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"fm_CPYL2nDHW"},"source":["Để kiểm tra các schema có trong một table chúng ta sử dụng hàm `printSchema()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":132,"status":"ok","timestamp":1684072215789,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"wEuJRyoRnDHW","outputId":"07d10471-4499-4369-e2f1-2330059ce1cd"},"outputs":[],"source":["flights.printSchema()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KXPdyaGPnDHX"},"source":["## 2.3. Thêm một spark DataFrame lưu trữ local vào một catalog\n","\n","Tuy nhiên lúc này flights vẫn chỉ là một spark DataFrame chưa có trong catalog của của cluster. Sử dụng hàm `listTable()` liệt kê danh sách bảng ta thu được 1 list rỗng."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":107,"status":"ok","timestamp":1684072215790,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"fMvBrIwFnDHX","outputId":"6f4e16ac-30ea-44a9-93e9-88ee55670265"},"outputs":[],"source":["# list all table exist in spark session\n","print(my_spark.catalog.listTables())"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-MdGccwQnDHX"},"source":["Lý do là bởi khi được đọc từ hàm `read.csv()` thì dữ liệu chỉ được lưu trữ ở Local dưới dạng một spark DataFrame. Để đưa dự liệu từ local lên cluster chúng ta cần save nó dưới dạng một `temporary table` thông một trong những lệnh bên dưới:\n","\n","* **.createTempView():** là một phương thức của spark DataFrame trong đó tham số duy nhất được truyền vào là tên bảng mà bạn muốn lưu trữ dưới dạng temporary table. Bảng được tạo ra là tạm thời và chỉ có thể được truy cập từ session được sử dụng để tạo ra spark DataFrame.\n","\n","* **.createOrReplaceTempView():** Có tác dụng hoàn toàn giống như `.createTempView()` nhưng nó sẽ update lại temporary table nếu nó đã tồn tại hoặc tạo mới nếu chưa tồn tại trước dây. Tránh trường hợp duplicate dữ liệu.\n","\n","Ngoài ra chúng ta còn sử dụng:\n","\n","* **.createDataFrame():** Tạo một spark DataFrame từ một pandas DataFrame.\n","\n","Để hiểu rõ hơn về nguyên tắc khởi tạo bảng chúng ta có thể xem sơ đồ bên dưới.\n","\n","![](https://s3.amazonaws.com/assets.datacamp.com/production/course_4452/datasets/spark_figure.png)\n","\n","> **Hình 2:** Sơ đồ chuyển đổi giữa spark DataFrame và catalog. Tử sơ đồ trên ta có thể thấy Spark Cluster sẽ tương tác với user thông qua kết nối từ SparkContext. Có 2 dạng lưu trữ chính ở SparkContext là spark DataFrame và catalog. Trong đó spark DataFrame là định dạng dạng bảng được lưu trữ ở Local và catalog là các temporary table sống trong các SparkSession. Để dữ liệu có thể truy cập từ một session chúng ta cần chuyển nó từ định dạng spark DataFrame sang temporary table thông qua hàm `.createTempView()` hoặc `.createOrReplaceTempView()`.\n","\n","Bên dưới ta sẽ khởi tạo một temporary table với tên là flights_temp cho bảng flights."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":503,"status":"ok","timestamp":1684072216225,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"odGc1WZ7nDHY","outputId":"7e73f18e-974a-4452-db7b-9cabc37e8a91"},"outputs":[],"source":["# Create a temporary table on catalog of local data frame flights as new temporary table flights_temp on catalog\n","flights.createOrReplaceTempView('flights_temp')\n","# check list all table available on catalog\n","my_spark.catalog.listTables()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Dshuk3Q9nDHY"},"source":["## 2.4. Các lệnh biến đổi dữ liệu của spark DataFrame\n","\n","Thông thường sẽ gồm các lệnh chính như tạo trường, update trường, xóa trường. Các lệnh bên dưới đều là các thuộc tính của spark DataFrame.\n","\n","* **.withColumn(\"newColumnName\", formular):** Thêm một trường mới vào một bảng sẵn có. Gồm 2 tham số chính, tham số thứ nhất là tên trường mới, tham số thứ 2 là công thức cập nhật tên trường. Lưu ý rằng spark DataFrame là một dạng dữ liệu immutable (không thể modified được). Do đó ta không thể inplace update (như các hàm `fillna()` hoặc `replace()` của pandas dataframe) mà cần phải gán giá trị trả về vào chính tên bảng ban đầu để cập nhật trường mới.\n","\n","Chẳng hạn bên dưới ta sẽ thêm 1 trường mới là HOUR_ARR được tính dựa trên AIR_TIME/60 (qui từ phút ra h) của bảng flights như sau:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684072216226,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"00idr1cbnDHY","outputId":"df0634f5-c26d-46ed-dc81-8f290149c913"},"outputs":[],"source":["flights = flights.withColumn('HOUR_ARR', flights.AIR_TIME/60)\n","flights.printSchema()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"rI-n776anDHY"},"source":["* **.withColumnRenamed(\"oldColumnName\", \"newColumnName\"):** Đổi tên của một column name trong pandas DataFrame."]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"OCzCLi5OnDHZ"},"source":["* **.select(\"column1\", \"column2\", ... , \"columnt\", formular):** Lựa chọn danh sách các trường trong spark DataFrame thông qua các tên column được truyền vào đưới dạng string và tạo ra một trường mới thông qua formular. Lưu ý để đặt tên cho trường mới ứng với formular chúng ta sẽ cần sử dụng hàm `formula.alias(\"columnName\")`.\n","\n","Bên dưới chúng ta sẽ tạo ra trường `avg_speed` tính vận tốc trung bình của các máy bay bằng cách lấy khoảng cách (DISTANCE) chi cho thời gian bay (HOUR_ARR) group by theo mã máy bay (TAIL_NUMBER) bằng lệnh select."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":534,"status":"ok","timestamp":1684072216757,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"D4Y0lnIsnDHZ","outputId":"19ad45cb-32aa-4d24-9ab8-17e1c7e5257f"},"outputs":[],"source":["avg_speed = flights.select(\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\", \"TAIL_NUMBER\", (flights.DISTANCE/flights.HOUR_ARR).alias(\"avg_speed\"))\n","avg_speed.printSchema()\n","avg_speed.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ZcSIrwkjnDHZ"},"source":["* **.selectExpr(\"column1\", \"column2\", ... , \"columnt\", \"formularExpr\"):** Hoàn toàn tương tự như `.select()` nhưng tham số formular được thay thế bằng chuỗi string biểu diễn công thức như trong câu lệnh SQL."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":433,"status":"ok","timestamp":1684072217187,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"PAF8T94bnDHa","outputId":"7a914b66-fc3a-4aa2-cda5-f2bc04ed7321"},"outputs":[],"source":["avg_speed_exp = flights.selectExpr(\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\", \"TAIL_NUMBER\", \"(DISTANCE/HOUR_ARR) AS avg_speed\")\n","avg_speed_exp.printSchema()\n","avg_speed_exp.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"xp9ucMOdnDHa"},"source":["* **.filter(condition):** Lọc một bảng theo một điều kiện nào đó. Condition có thể làm một string expression biểu diễn công thức lọc hoặc một công thức giữa các trường trong spark DataFrame. Lưu ý Condition phải trả về một trường dạng Boolean type.\n","\n","Chẳng hạn chúng ta muốn lọc những chuyến bay xuất phát từ sân bay `SEA` và điểm đến là `ANC` ta có thể sử dụng filter như sau:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":1270,"status":"ok","timestamp":1684072218451,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"UUjbG-LQnDHa","outputId":"ca6c18db-df0f-4d78-8a3b-bbe01a520535"},"outputs":[],"source":["filter_SEA_ANC = flights.filter(\"ORIGIN_AIRPORT == 'SEA'\") \\\n","                        .filter(\"DESTINATION_AIRPORT == 'ANC'\")\n","filter_SEA_ANC.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"TQRB-cIWnDHb"},"source":["* **.groupBy(\"column1\", \"column2\",...,\"columnt\"):** Tương tự như lệnh GROUP BY của SQL, lệnh này sẽ nhóm các biến theo các dimension được truyền vào groupBy. Theo sau lệnh `groupBy()` là một build-in function của spark DataFrame được sử dụng để tính toán theo một biến đo lường nào đó chẳng hạn như hàm `avg()`, `min()`, `max()`, `sum()`.  Tham số được truyền vào các hàm này chính là tên biến đo lường.\n","\n","Bên dưới chúng ta sẽ tính thời gian bay trung bình theo điểm xuất phát (ORIGIN_AIRPORT)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":23275,"status":"ok","timestamp":1684072241716,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"5_JKeaTWnDHb","outputId":"3e7d283b-fab4-43e4-ca5d-7ffebfffca0e"},"outputs":[],"source":["avg_time_org_airport = flights.groupBy(\"ORIGIN_AIRPORT\").avg(\"HOUR_ARR\")\n","avg_time_org_airport.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"zihHTICPnDHb"},"source":["* **.join(tableName, on = \"columnNameJoin\", how = \"leftouter\"):** Join 2 bảng với nhau tương tự như lệnh left join trong SQL. Kết quả trả về sẽ là các trường mới trong bảng `tableName` kết hợp với các trường cũ trong bảng gốc thông qua key là. Lưu ý rằng `columnNameJoin` phải trùng nhau giữa 2 bảng.  "]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"fOLrfNk2nDHc"},"source":["## 2.6. Các lệnh biến đổi dữ liệu của spark.sql()\n","\n","Một trong những lợi thế khi đưa bảng lên cluster đó là có thể sử dụng các câu lệnh biến đổi SQL từ phương thức `.sql()` của spark để transform dữ liệu. Bên dưới là các lệnh cơ bản mà chúng sẽ có thể gặp:\n","\n","* **SELECT**: Có cú pháp chung là: `SELECT * FROM TABLENAME WHERE CONDTION`\n","Lệnh này sẽ lựa chọn các trường trong bảng theo điều kiện tại WHERE. Đây là lệnh rất quen thuộc trong SQL.\n","\n","Chẳng hạn bên dưới chúng ta lấy ra những chuyến bay có thời gian bay > 10 phút."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":116,"status":"ok","timestamp":1684072241718,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"oqLI5v1inDHc","outputId":"35f87a16-2834-4aa9-f40d-8311d40d0345"},"outputs":[],"source":["flights_10 = my_spark.sql('SELECT * FROM flights_temp WHERE AIR_TIME > 10')\n","flights_10.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Z8j4gPMAnDHc"},"source":["* **GROUP BY:** Lệnh này sẽ thống kê các các trường measurement (các trường dùng để tính toán) theo một nhóm các trường dimension (các trường dùng để phân loại) dựa trên các aggregation function như `sum(), avg(), min(), max(), mean(), median(), count(), count(distinct())` của SQL.\n","\n","Chẳng hạn chúng ta muốn tính số phút bay trung bình của mỗi hãng bay trong năm sẽ sử dụng hàm GROUP BY như sau:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":27967,"status":"ok","timestamp":1684072269585,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"ncBMyma0nDHd","outputId":"d68361ad-50a6-4e96-866a-e5cfda035813"},"outputs":[],"source":["print(my_spark.catalog.listTables())\n","agg_arr_time = my_spark.sql(\"SELECT ORIGIN_AIRPORT, DESTINATION_AIRPORT, TAIL_NUMBER, MEAN(AIR_TIME) AS avg_speed FROM flights_temp GROUP BY ORIGIN_AIRPORT, DESTINATION_AIRPORT, TAIL_NUMBER\")\n","agg_arr_time.show(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"yExU5nK3nDHd"},"source":["Ngoài ra ta còn có thể sử dụng vô số các lệnh SQL khác đối với các bảng xuất hiện trong catalog của clusters.\n","\n","* **UPDATE**: Cập nhật một trường theo một công thức nào đó.\n","\n","* **INSERT**: Insert thêm row mới cho bảng.\n","\n","* **DELETE**: Xóa các records của bảng theo điều kiện.\n","\n","* **Nhóm các lệnh join**: gồm các lệnh `left join, right join, inner join, outer join`.\n","\n","Các ví dụ về lệnh này trên SQL các bạn có thể xem [SQL tutorial - Website W3School](https://www.w3schools.com/sql/).\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5GtNogydnDHd"},"source":["# 3. Xây dựng pipeline End-to-End model trên pyspark\n","\n","## 3.1. Xây dựng pipeline biến đổi dữ liệu.\n","\n","pyspark cho phép xây dựng các end-to-end model mà dữ liệu truyền vào là các raw data và kết quả trả ra là nhãn, xác xuất hoặc giá trị được dự báo của model. Các end-to-end model này được đi qua một pipe line của \n","\n","pyspark.ml bao gồm 2 class cơ bản là `Transformer` cho phép biến đổi dữ liệu và `Estimator` ước lượng mô hình dự báo. \n","\n","* Transfromer sử dụng hàm `.transform()` nhận đầu vào là 1 DataFrame và trả ra một DataFrame mới có các trường đã biến đổi theo Transform. Các bạn sẽ hiểu hơn qua thực hành ở ví dụ bên dưới.\n","\n","* Estimator sử dụng hàm `.fit()` để huấn luyện model. Chúng cũng nhận đầu vào là một DataFrame nhưng kết quả được trả ở đầu ra là 1 model object. Hiện tại spark hỗ trợ khá nhiều các lớp model cơ bản trong machine learning. Các lớp model xuất hiện trong Esimator bao gồm:\n","\n","    * **Đối với bài toán phân loại:** `LogisticRegression, DecisionTreeClassifier, RandomForestModel, GBTClassifier (gradient bosting tree), MultilayerPerceptronClassifier, LinearSVC (Linear Support Vector Machine), NaiveBayes`.\n","\n","    * **Đối với bài toán dự báo:** `GeneralizedLinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor (gradient boosting Tree), AFTSurvivalRegression (Hồi qui đối với các lớp bài toán estimate survival)`.\n","\n","Cách thức áp dụng các model này các bạn có thể xem hướng dẫn rất chi tiết tại trang chủ của [Spark apache](https://spark.apache.org/docs/latest/ml-classification-regression.html).\n","\n","Bên dưới chúng ta sẽ cùng đi xây dựng 1 pipeline cho model dự báo khả năng trễ chuyến bay dựa trên dữ liệu đầu vào là bảng flights. Trước tiên chúng ta cùng tìm hiểu các trường trong bảng dữ liệu."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":47,"status":"ok","timestamp":1684072269588,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"8awNU3n_nDHd","outputId":"d376a408-17ff-402e-de53-11532a42b6a3"},"outputs":[],"source":["flights.printSchema()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"GM9R7n_ynDHe"},"source":["Ý nghĩa các trường như sau:\n","\n","* YEAR: string (nullable = true): Năm.\n","* MONTH: string (nullable = true): Tháng.\n","* DAY: string (nullable = true): Ngày.\n","* DAY_OF_WEEK: string (nullable = true): Ngày trong tuần.\n","* AIRLINE: string (nullable = true): Hãng hàng không.\n","* FLIGHT_NUMBER: string (nullable = true): Mã chuyến bay.\n","* TAIL_NUMBER: string (nullable = true): Số hiệu máy bay.\n","* ORIGIN_AIRPORT: string (nullable = true): Nơi xuất phát.\n","* DESTINATION_AIRPORT: string (nullable = true): Điểm đến.\n","* SCHEDULED_DEPARTURE: string (nullable = true): Lịch trình xuất phát.\n","* DEPARTURE_TIME: string (nullable = true): Thời gian xuất phát thực tế.\n","* DEPARTURE_DELAY: string (nullable = true): Thời gian bị trễ.\n","* TAXI_OUT: string (nullable = true): Thời gian taxi ra.\n","* WHEELS_OFF: string (nullable = true): Thời gian lăn bánh cất cánh.\n","* SCHEDULED_TIME: string (nullable = true): Thời gian theo lịch trình.\n","* ELAPSED_TIME: string (nullable = true): Không rõ.\n","* AIR_TIME: string (nullable = true): Thời gian cất cánh.\n","* DISTANCE: string (nullable = true): Khoảng cách.\n","* WHEELS_ON: string (nullable = true): Thời gian lăn bánh hạ cánh.\n","* TAXI_IN: string (nullable = true): thời gian taxi vào\n","* SCHEDULED_ARRIVAL: string (nullable = true): Thời gian theo lịch di chuyển.\n","* ARRIVAL_TIME: string (nullable = true): Thời gian di chuyển.\n","* ARRIVAL_DELAY: string (nullable = true): Thời gian trễ.\n","* DIVERTED: string (nullable = true): Chuyển hướng.\n","* CANCELLED: string (nullable = true): Hủy chuyến.\n","* CANCELLATION_REASON: string (nullable = true): Lý do hủy.\n","* AIR_SYSTEM_DELAY: string (nullable = true): Trễ vì hệ thống hàng không.\n","* SECURITY_DELAY: string (nullable = true): Trễ vì lý do an ninh.\n","* AIRLINE_DELAY: string (nullable = true): Trễ vì lý do từ hãng.\n","* LATE_AIRCRAFT_DELAY: string (nullable = true): Trễ vì phi cơ.\n","* WEATHER_DELAY: string (nullable = true): Trễ vì thời tiết.\n","* HOUR_ARR: double (nullable = true): Số h di chuyển.\n","\n","Vì là ví dụ demo nên để giảm thiểu thời gian tính toán tôi sẽ chỉ lấy dữ liệu của những chuyến bay xuất phát từ 'SEA' của hãng delta airline và american airlines ('DA' và 'AA'). Dữ liệu dự báo bao gồm các trường: ARRIVAL_DELAY, ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK, DESTINATION_AIRPORT, AIRLINE. Trong đó trường ARRIVAL_DELAY > 0  xác định chuyến bay trễ và ARRIVAL_DELAY = 0 chuyến bay không bị trễ.\n","\n","\n","Do các mô hình chỉ nhận đầu vào là các biến numeric nên ta phải có các bước xử lý dữ liệu từ biến string, boolean sang biến numeric.\n","\n","Chúng ta sẽ phân các biến trên thành 2 nhóm biến là:\n","\n","* **Các biến numeric:** ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK. Không cần phải qua biến đổi và được sử dụng trực tiếp làm đầu vào của model hồi qui. Tuy nhiên các biến này đang được để ở dạng string nên phải chuyển qua numeric bằng hàm CAST.\n","\n","* **Các biến string:** DESTINATION_AIRPORT, AIRLINE là các biến dạng category cần được đánh index và biến đổi sang dạng biến dummies (chỉ nhận giá trị 0 và 1) để có thể đưa vào model hồi qui. Khi đó mỗi một biến sẽ được phân thành nhiều features mà mỗi một features đại diện cho 1 nhóm của biến. Quá trình biến đổi dummies sẽ trải qua 2 bước: Đánh index cho biến bằng class `StringIndexer()`. Một index sẽ được gán cho 1 nhóm biến. Biểu diễn one-hot vector thông qua class `OneHotEncoder()`: Từ index của biến sẽ biểu diễn các biến dưới dạng one-hot vector sao cho vị trí bằng 1 sẽ là phần tử có thứ tự trùng với index. Cả 2 class `StringIndexer()` và `OneHotEncoder()` đều là các object của `pyspark.ml.feature`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":18921,"status":"ok","timestamp":1684072288481,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"rHOrkQbunDHe","outputId":"79c400d0-f8a9-47b5-daa4-32a248d98aa3"},"outputs":[],"source":["print('Shape of previous data: ({}, {})'.format(flights.count(), len(flights.columns)))\n","flights_SEA = my_spark.sql(\"select ARRIVAL_DELAY, ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK, DESTINATION_AIRPORT, AIRLINE from flights_temp where ORIGIN_AIRPORT = 'SEA' and AIRLINE in ('DL', 'AA') \")\n","print('Shape of flights_SEA data: ({}, {})'.format(flights_SEA.count(), len(flights_SEA.columns)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":16707,"status":"ok","timestamp":1684072305102,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"TWiZQj1znDHe","outputId":"df63fcc7-d629-40d4-f3c1-37f2d56e581e"},"outputs":[],"source":["# Create boolean variable IS_DELAY variable as Target\n","flights_SEA = flights_SEA.withColumn(\"IS_DELAY\", flights_SEA.ARRIVAL_DELAY > 0)\n","# Now Convert Boolean variable into integer\n","flights_SEA = flights_SEA.withColumn(\"label\", flights_SEA.IS_DELAY.cast(\"integer\"))\n","# Remove missing value\n","model_data = flights_SEA.filter(\"ARRIVAL_DELAY is not null \\\n","                                and ARRIVAL_TIME is not null \\\n","                                and MONTH is not null \\\n","                                and YEAR is not null  \\\n","                                and DAY_OF_WEEK is not null \\\n","                                and DESTINATION_AIRPORT is not null \\\n","                                and AIRLINE is not null\")\n","\n","print('Shape of model_data data: ({}, {})'.format(model_data.count(), len(model_data.columns)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1684072305106,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"f4DHn0UK4XGq","outputId":"138d5260-fc49-4c1f-f803-2c291cfc932e"},"outputs":[],"source":["flights_SEA.take(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uxHVDy81nDHf"},"source":["Một chú ý quan trọng đó là các model phân loại của pyspark luôn mặc định nhận biến dự báo là label. Do đó trong bất kì model nào chúng ta cũng cần tạo ra biến integer là nhãn của model dưới tên label."]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"yUTdCszfnDHf"},"source":["Convert các biến string sang numeric bằng hàm `.withColumn()`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":true,"editable":true,"executionInfo":{"elapsed":142,"status":"ok","timestamp":1684072305109,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"IDMxxL5ynDHf","outputId":"7565a20f-ad55-4187-b79e-ad4beaa90d9a"},"outputs":[],"source":["# ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK\n","model_data = model_data.withColumn(\"ARRIVAL_TIME\", model_data.ARRIVAL_TIME.cast(\"integer\"))\n","model_data = model_data.withColumn(\"MONTH\", model_data.MONTH.cast(\"integer\"))\n","model_data = model_data.withColumn(\"YEAR\", model_data.YEAR.cast(\"integer\"))\n","model_data = model_data.withColumn(\"DAY_OF_WEEK\", model_data.DAY_OF_WEEK.cast(\"integer\"))\n","model_data.printSchema()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"_1YRH_mOnDHg"},"source":["Biến đổi các biến String bằng StringIndexer và OneHotEncoder."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"executionInfo":{"elapsed":950,"status":"ok","timestamp":1684072305958,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"HCl6XcB7nDHg"},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder\n","# I. With DESTINATION_AIRPORT\n","# Create StringIndexer\n","dest_indexer = StringIndexer(inputCol = \"DESTINATION_AIRPORT\", \\\n","                             outputCol = \"DESTINATION_INDEX\")\n","\n","# Create OneHotEncoder\n","dest_onehot = OneHotEncoder(inputCol = \"DESTINATION_INDEX\", \\\n","                            outputCol = \"DESTINATION_FACT\")\n","\n","# II. With AIRLINE\n","# Create StringIndexer\n","airline_indexer = StringIndexer(inputCol = \"AIRLINE\", \\\n","                                outputCol = \"AIRLINE_INDEX\")\n","\n","# Create OneHotEncoder\n","airline_onehot = OneHotEncoder(inputCol = \"AIRLINE_INDEX\", \\\n","                               outputCol = \"AIRLINE_FACT\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"uaVl26OTnDHg"},"source":["Đầu ra của quá trình biến đổi trên 2 biến DESTINATION_AIRPORT và AIRLINE chính là biến DESTINATION_FACT và AIRLINE_FACT. Hai biến này sẽ cần được đưa vào vector tổng hợp sẽ được giới thiệu biên dưới. Các objects: dest_indexer, dest_onehot, airline_indexer, airline_onehot sẽ được truyền vào pipeline theo thứ tự để thể hiện quá trình transform dữ liệu tuần tự từ trái qua phải.\n","\n","Bước cuối cùng của pipeline là kết hợp toàn bộ các columns chứa các features thành một column duy nhất. Bước này phải được thực hiện trước khi model training bởi spark model sẽ chỉ chấp nhận đầu vào ở định dạng này. Chúng ta có thể lưu mỗi một giá trị từ một cột như một phần tử của vector. Khi đó vector sẽ chứa toàn bộ các thông tin cần thiết của 1 quan sát để xác định nhãn hoặc giá trị dự báo ở đầu ra của quan sát đó. Class `VectorAssembler` của pyspark.ml.feature sẽ tạo ra vector tổng hợp đại diện cho toàn bộ các chiều của quan sát đầu vào. Việc chúng ta cần thực hiện chỉ là truyền vào class list string tên các trường thành phần của vector tổng hợp."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1684072305967,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"Zc_UBIRRnDHg"},"outputs":[],"source":["# Make a VectorAssembler\n","from pyspark.ml.feature import VectorAssembler\n","vec_assembler = VectorAssembler(inputCols = [\"ARRIVAL_TIME\", \"MONTH\", \"YEAR\", \\\n","                                             \"DAY_OF_WEEK\", \"DESTINATION_FACT\",\\\n","                                             \"AIRLINE_FACT\"], \n","                                outputCol = \"features\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NGFm5kJSnDHh"},"source":["Sau stage này chúng ta sẽ tổng hợp các biến trong inputCols thành một vector dự báo ở outputCol được lưu dưới tên `features`. Nhãn của model luôn mặc định là biến `label` đã khởi tạo từ đầu.\n","\n","Tiếp theo chúng ta sẽ khởi tạo pipeline biến đổi dữ liệu cho model thông qua class `Pipeline` của `pyspark.ml`. Các transformer biến đổi dữ liệu sẽ được sắp xếp trong 1 list và truyền vào tham số stages như bên dưới."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"executionInfo":{"elapsed":34,"status":"ok","timestamp":1684072305971,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"0gq-386vnDHh"},"outputs":[],"source":["from pyspark.ml import Pipeline\n","\n","# Make a pipeline\n","flights_sea_pipe  = Pipeline(stages = [dest_indexer, dest_onehot, airline_indexer, \\\n","                                       airline_onehot, vec_assembler])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"J8bKa3_tnDHh"},"source":["Sau khi dữ liệu được `fit()` qua pipeline sẽ thu được đầu ra là vector tổng hợp các biến dự báo `features` và nhãn của quan sát `label`."]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Vt1r4XPLnDHh"},"source":["## 3.2. Huấn luyện và đánh giá model.\n","\n","### 3.2.1. Phân chia tập train/test.\n","\n","Sau khi đi qua pipe line ở bước Transform dữ liệu chúng ta sẽ thu được vector tổng hợp (Vector Assemble) và nhãn (LABLE) ở đầu ra. Tiếp theo tại bước này sẽ thực hiện phân chia tập train/test theo tỷ lệ 80%/20%. Trong đó model được xây dựng trên tập Train và được đánh giá trên tập Test. Tại sao lại phải để riêng 1 tập test mà không hồi qui trên toàn bộ dữ liệu? Đó là vì model thường phân loại hoặc dự báo tốt trên dữ liệu nó đã được huấn luyện mà không phân loại, dự báo tốt đối với dữ liệu mới. Chính vì thế tập test được coi như dữ liệu mới không được sử dụng trong huấn luyện và dùng để đánh giá khả năng dự báo của model."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"executionInfo":{"elapsed":33512,"status":"ok","timestamp":1684072339453,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"68_XdtP4nDHh"},"outputs":[],"source":["# create pipe_data from pipeline\n","pipe_data = flights_sea_pipe.fit(model_data).transform(model_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"executionInfo":{"elapsed":50,"status":"ok","timestamp":1684072339454,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"Cme9LPwHnDHi"},"outputs":[],"source":["# Split train/test data\n","train, test = pipe_data.randomSplit([0.8, 0.2])"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"N90Ik1L-nDHi"},"source":["### 3.2.2. Huấn luyện và đánh giá model.\n","\n","Có rất nhiều các lớp model phân loại đã được giới thiệu trong mục 3.1. Để đơn giản hóa chúng ta sẽ chọn ra model LogisticRegression trong ví dụ demo này làm model phân loại."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"executionInfo":{"elapsed":48,"status":"ok","timestamp":1684072339454,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"_PZxAHf7nDHi"},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","\n","# Create logistic regression\n","lr = LogisticRegression()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vDKe6KOpnDHi"},"source":["Để đánh giá model chúng ta cần sử dụng các metric như `ROC, Accuracy, F1, precision hoặc recal`. Do dữ liệu không có hiện tượng mất cân bằng giữa 2 lớp nên ta sẽ sử dụng ROC làm metric đánh giá model. Trong trường hợp mẫu mất cân bằng thì các chỉ số `F1, precision hoặc recal` nên được sử dụng thay thế vì trong tình huống này ROC, Accuracy thường mặc định là rất cao. Chúng ta sử dụng class `BinaryClassificationEvaluator` trong `pyspark.ml.evaluation` module để tính toán các metrics đánh giá model."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"executionInfo":{"elapsed":47,"status":"ok","timestamp":1684072339455,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"Au5PnulLnDHi"},"outputs":[],"source":["# Import the evaluation submodule\n","import pyspark.ml.evaluation as evals\n","\n","# Create a BinaryClassificationEvaluator\n","evaluator = evals.BinaryClassificationEvaluator(metricName = \"areaUnderROC\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"LFb8rwJenDHj"},"source":["### 3.2.3. Tuning model thông qua param gridSearch.\n","\n","pyspark cho phép chúng ta tuning model trên một gridSearch các params. Tuning model được hiểu là tìm kiếm trong 1 không gian các tham số của model để thu được model mà giá trị của metric khai báo ở bước 3.2.2 thu được là nhỏ nhất trên tập test. Để xây dựng một gridSearch chúng ta sử dụng class ParamGridBuilder của submodule pyspark.ml.tuning. Để thêm các gridSearch chúng ta sử dụng phương thức `.addGrid()` và sau đó sử dụng hàm `.build()` để khởi tạo gridSearch. Mỗi lớp model sẽ có các tham số khác nhau. Do đó để tuning được model cần phải xác định xem model có những tham số gì. Việc này đòi hỏi bạn phải nắm vững về thuật toán và phương pháp tối ưu của model. Mình sẽ không đi sâu vào phần này. Đối với LogisticRegression mình sẽ lựa chọn lr.regParam là tham số tuning. Một lưu ý khác là việc tuning có thể rất tốt thời gian và tài nguyên của máy tính do thực hiện nhiều model của cùng 1 lớp model nhưng với các tham số khau. Do đó bạn cần cân nhắc cấu hình máy trước khi thực hiện."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"executionInfo":{"elapsed":47,"status":"ok","timestamp":1684072339456,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"},"user_tz":-420},"id":"WmETDdWYnDHj"},"outputs":[],"source":["# Import the tuning submodule\n","import pyspark.ml.tuning as tune\n","import numpy as np\n","\n","# Create the parameter grid\n","grid = tune.ParamGridBuilder()\n","\n","# Add the hyperparameter, we can add more than one hyperparameter\n","grid = grid.addGrid(lr.regParam, np.arange(0, 0.1, 0.01))\n","\n","# Build the grid\n","grid = grid.build()"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"f8Rd07ADnDHj"},"source":["Sau khi build xong gridSearch chúng ta cần Cross Validate toàn bộ các model trên tập các tham số khởi tạo ở `grid`. Nếu bạn đọc chưa nắm rõ về Cross Validate là gì xin mời đọc bài viết [Cross validation tuning threshold](http://rpubs.com/phamdinhkhanh/383309) của mình. Chúng ta cần khởi tạo một cross validator từ class CrossValidator của pyspark.ml.tuning."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"UYYku4NNnDHj"},"outputs":[],"source":["# Create the CrossValidator\n","cv = tune.CrossValidator(estimator=lr,\n","               estimatorParamMaps=grid,\n","               evaluator=evaluator\n","               )\n","\n","# Fit cross validation on models\n","models = cv.fit(train)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2PRAIpQWnDHj"},"source":["Mình sẽ không chạy lệnh này do máy cấu hình yếu. Sau khi tuning xong chúng ta sẽ thu được model tốt nhất thông qua hàm:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"i8MsTCbPnDHk"},"outputs":[],"source":["best_lr = models.bestModel"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"3l1mmI0rnDHk"},"source":["Do không chạy tuning nên mình sẽ gán cho best_lr model chính là model LogisticRegression với giá trị mặc định của regParam = 0."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"xEQvVXnLnDHk"},"outputs":[],"source":["best_lr = lr.fit(train)\n","print(best_lr)"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vwP8KPORnDHk"},"source":["Lưu ý để dự báo từ các model của pyspark, chúng ta không sử dụng hàm `predict()` như thông thường mà sử dụng hàm `transform()`. Kiểm tra mức độ chính xác của model trên tập test bằng hàm `evaluate()`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"1NU0-iJInDHl"},"outputs":[],"source":["# Use the model to predict the test set\n","test_results = best_lr.transform(test)\n","\n","# Evaluate the predictions\n","print(evaluator.evaluate(test_results))"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"P-NY0Cq1nDHl"},"source":["Như vậy trên tập test model đạt mức độ chính xác khoảng 58.9%. Phương án tốt được cân nhắc là thêm biến để cải thiện model.\n","\n","# 4. Tổng kết\n","\n","Như vậy thông qua bài viết này chúng ta đã được nắm bắt được hiệu quả của spark trong xử lý dữ liệu lớn. Tôi xin tổng hợp lại các kiến thức thu được trong bài:\n","\n","1. Cách thức khởi tạo một spark context và quản lý các table trên catalog của cluster.\n","2. Các lệnh cơ bản biến đổi và tổng hợp dữ liệu trên spark DataFrame.\n","3. Các lệnh SQL được tích hợp trên spark.\n","4. Tạo pipeline biến đổi dữ liệu cho model phân loại.\n","5. Xây dựng và đánh giá model trên pyspark.\n","\n","Ngoài ra còn rất nhiều các kiến thức về thiết lập cụm và xử lý tính toán khác tôi sẽ giới thiệu ở những bài tiếp theo. Hi vọng những kiến thức này sẽ giúp ích bạn đọc trong việc xây dựng các model với dữ liệu lớn. Bài viết được tổng hợp từ rất nhiều các nguồn kiến thức trên mạng mà tôi sẽ liệt kê bên đưới.\n","\n","# 5. Tài liệu\n","\n","1. [SparkSQL guideline - Spark homepage](https://spark.apache.org/docs/2.3.1/sql-programming-guide.html)\n","2. [Spark Machine Learning Lib - Spark homepage](https://spark.apache.org/docs/2.3.1/ml-guide.html)\n","3. [Spark structured streaming data - Spark homepage](https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html)\n","4. [Bigdata essentials - coursera](https://www.coursera.org/lecture/big-data-essentials/getting-started-with-spark-python-o6oKt)\n","5. [PySpark Tutorial for Beginners - youtube](https://www.youtube.com/watch?v=639JCua-bQU)\n","6. [CCA 175 - Spark and Hadoop Developer - udemy](https://www.udemy.com/cca-175-spark-and-hadoop-developer-python-pyspark/)\n","7. [Introduction to spark - datacamp](https://campus.datacamp.com/courses/introduction-to-pyspark)\n","8. [Spark and Python for Big Data with PySpark - udemy](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
