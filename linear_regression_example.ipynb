{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:00:37 WARN Utils: Your hostname, lambda-server resolves to a loopback address: 127.0.1.1; using 192.168.1.252 instead (on interface eno1)\n",
      "23/07/24 18:00:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/24 18:00:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc have not yet created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd    \n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/workspace/tripx/MCS/big_data/spark-3.1.1-bin-hadoop3.2\"\n",
    "findspark.init()\n",
    "import time \n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, count, explode, sum as sum_\n",
    "\n",
    "\n",
    "# Stop spark if it existed\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '100g')\n",
    "# SparkContext.setSystemProperty('spark.driver.maxResultsSize', '0')\n",
    "# SparkContext.setSystemProperty('spark.driver.memory', '100g')\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('sc have not yet created!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:00:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/07/24 18:00:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init session\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:00:46 WARN DAGScheduler: Broadcasting large task binary with size 1257.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:00:48 WARN DAGScheduler: Broadcasting large task binary with size 1257.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master = \"local\", appName = \"Multimodal Single Cell\")\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# Init session\n",
    "print(\"Init session\")\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "print(my_spark.catalog.listTables())\n",
    "\n",
    "train_cite_inputs_path = \"/workspace/tripx/MCS/big_data/data/sub_train_input.csv\"\n",
    "train_cite_inputs_data = my_spark.read.csv(train_cite_inputs_path,  header=True, maxColumns=30000)\n",
    "\n",
    "train_cite_target_path = \"/workspace/tripx/MCS/big_data/data/sub_train_output.csv\"\n",
    "train_cite_target_data = my_spark.read.csv(train_cite_target_path,  header=True)\n",
    "\n",
    "print(train_cite_inputs_data.count())\n",
    "print(train_cite_inputs_data.count())\n",
    "\n",
    "new_cols=(column.replace('.', '_') for column in train_cite_inputs_data.columns)\n",
    "new_train_data = train_cite_inputs_data.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cite_inputs_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = train_cite_inputs_data.select(\"cell_id\", \"ENSG00000121410_A1BG\", \"ENSG00000175899_A2M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSG00000121410_A1BG\n",
      "ENSG00000175899_A2M\n"
     ]
    }
   ],
   "source": [
    "columns = ['ENSG00000121410_A1BG', 'ENSG00000175899_A2M']\n",
    "for col_name in columns:\n",
    "    print(col_name)\n",
    "    new_train_data = new_train_data.withColumn(col_name,  col(col_name).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cell_id: string (nullable = true)\n",
      " |-- ENSG00000121410_A1BG: float (nullable = true)\n",
      " |-- ENSG00000175899_A2M: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_label_data = train_cite_target_data.select(\"cell_id\", \"CD86\", \"CD274\", 'CD270')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD86\n",
      "CD274\n",
      "CD270\n"
     ]
    }
   ],
   "source": [
    "for col_name in [\"CD86\", \"CD274\", 'CD270']:\n",
    "    print(col_name)\n",
    "    new_train_label_data = new_train_label_data.withColumn(col_name,  col(col_name).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cell_id: string, CD86: float, CD274: float, CD270: float]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = new_train_data.merge(new_train_label_data, left_on='cell_id', right_on='cell_id' )\n",
    "\n",
    "train_data = new_train_data.join(new_train_label_data, new_train_data.cell_id ==  new_train_label_data.cell_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------------+------------+------------+-----------+-----------+\n",
      "|     cell_id|ENSG00000121410_A1BG|ENSG00000175899_A2M|     cell_id|        CD86|      CD274|      CD270|\n",
      "+------------+--------------------+-------------------+------------+------------+-----------+-----------+\n",
      "|45006fe3e4c8|                 0.0|                0.0|45006fe3e4c8|   1.1678035|    0.62253| 0.10695851|\n",
      "|d02759a80ba2|                 0.0|                0.0|d02759a80ba2|  0.81897014| 0.50600946|   1.078682|\n",
      "|c016c6b0efa5|                 0.0|                0.0|c016c6b0efa5|  -0.3567033|-0.42226133|-0.82449275|\n",
      "|ba7f733a4f75|                 0.0|                0.0|ba7f733a4f75|  -1.2015074| 0.14911485|  2.0224676|\n",
      "|fbcf2443ffb2|                 0.0|                0.0|fbcf2443ffb2|-0.100404024|  0.6974609| 0.62583566|\n",
      "+------------+--------------------+-------------------+------------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:37:23 WARN DAGScheduler: Broadcasting large task binary with size 1260.1 KiB\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.select('ENSG00000121410_A1BG', 'ENSG00000175899_A2M', \"CD86\", \"CD274\", 'CD270')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ENSG00000121410_A1BG: float, ENSG00000175899_A2M: float, CD86: float, CD274: float, CD270: float]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler_features = VectorAssembler(inputCols = [\"ENSG00000121410_A1BG\", \"ENSG00000175899_A2M\"],\n",
    "                                outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler_label = VectorAssembler(inputCols = [\"CD86\", \"CD274\", 'CD270'],\n",
    "                                outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = vec_assembler_features.transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------+-----------+-----------+---------+\n",
      "|ENSG00000121410_A1BG|ENSG00000175899_A2M|        CD86|      CD274|      CD270| features|\n",
      "+--------------------+-------------------+------------+-----------+-----------+---------+\n",
      "|                 0.0|                0.0|   1.1678035|    0.62253| 0.10695851|(2,[],[])|\n",
      "|                 0.0|                0.0|  0.81897014| 0.50600946|   1.078682|(2,[],[])|\n",
      "|                 0.0|                0.0|  -0.3567033|-0.42226133|-0.82449275|(2,[],[])|\n",
      "|                 0.0|                0.0|  -1.2015074| 0.14911485|  2.0224676|(2,[],[])|\n",
      "|                 0.0|                0.0|-0.100404024|  0.6974609| 0.62583566|(2,[],[])|\n",
      "|                 0.0|                0.0|   0.8239842|  1.6257721|  1.8227521|(2,[],[])|\n",
      "|                 0.0|                0.0|  -0.2512331| 0.43728906| 0.44692641|(2,[],[])|\n",
      "|                 0.0|                0.0| -0.71294916| 0.76743567|  0.3196118|(2,[],[])|\n",
      "|                 0.0|                0.0|  -0.7890341|-0.96942055| -0.7789143|(2,[],[])|\n",
      "|                 0.0|                0.0| -0.47382092|  0.1555812|-0.37076998|(2,[],[])|\n",
      "|                 0.0|                0.0|   0.7749718|   0.125361| -0.6935526|(2,[],[])|\n",
      "+--------------------+-------------------+------------+-----------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:45:07 WARN DAGScheduler: Broadcasting large task binary with size 1271.8 KiB\n"
     ]
    }
   ],
   "source": [
    "input_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vector = vec_assembler_label.transform(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector = output_vector.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "| features|               label|\n",
      "+---------+--------------------+\n",
      "|(2,[],[])|[1.16780352592468...|\n",
      "|(2,[],[])|[0.81897014379501...|\n",
      "|(2,[],[])|[-0.3567033112049...|\n",
      "|(2,[],[])|[-1.2015074491500...|\n",
      "|(2,[],[])|[-0.1004040241241...|\n",
      "|(2,[],[])|[0.82398420572280...|\n",
      "|(2,[],[])|[-0.2512331008911...|\n",
      "|(2,[],[])|[-0.7129491567611...|\n",
      "|(2,[],[])|[-0.7890341281890...|\n",
      "|(2,[],[])|[-0.4738209247589...|\n",
      "|(2,[],[])|[0.77497178316116...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:50:34 WARN DAGScheduler: Broadcasting large task binary with size 1279.8 KiB\n"
     ]
    }
   ],
   "source": [
    "train_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_vector.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# # Make a pipeline\n",
    "# single_cell_pipe  = Pipeline(stages = [vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_data = single_cell_pipe.fit(new_train_data).transform(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = vec_assembler.transform(new_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+\n",
      "|ENSG00000121410_A1BG|ENSG00000175899_A2M| features|\n",
      "+--------------------+-------------------+---------+\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "|                 0.0|                0.0|(2,[],[])|\n",
      "+--------------------+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 18:15:42 WARN DAGScheduler: Broadcasting large task binary with size 1269.1 KiB\n"
     ]
    }
   ],
   "source": [
    "new_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cite_target_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n",
      "|        CD86|      CD274|      CD270|\n",
      "+------------+-----------+-----------+\n",
      "|   1.1678035|    0.62253| 0.10695851|\n",
      "|  0.81897014| 0.50600946|   1.078682|\n",
      "|  -0.3567033|-0.42226133|-0.82449275|\n",
      "|  -1.2015074| 0.14911485|  2.0224676|\n",
      "|-0.100404024|  0.6974609| 0.62583566|\n",
      "|   0.8239842|  1.6257721|  1.8227521|\n",
      "|  -0.2512331| 0.43728906| 0.44692641|\n",
      "| -0.71294916| 0.76743567|  0.3196118|\n",
      "|  -0.7890341|-0.96942055| -0.7789143|\n",
      "| -0.47382092|  0.1555812|-0.37076998|\n",
      "|   0.7749718|   0.125361| -0.6935526|\n",
      "+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_train_label_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
