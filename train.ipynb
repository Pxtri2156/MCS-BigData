{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd    \n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/workspace/tripx/MCS/big_data/spark-3.1.1-bin-hadoop3.2\"\n",
    "findspark.init()\n",
    "import time \n",
    "import logging\n",
    "import seaborn as sns\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, count, explode, sum as sum_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 00:44:48 WARN Utils: Your hostname, lambda-server resolves to a loopback address: 127.0.1.1; using 192.168.1.252 instead (on interface eno1)\n",
      "23/07/27 00:44:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/27 00:44:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc have not yet created!\n",
      "Init session\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Stop spark if it existed\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '200g')\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('sc have not yet created!')\n",
    "sc = SparkContext(master = \"local[*]\", appName = \"Multimodal Single Cell\")\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# Init session\n",
    "print(\"Init session\")\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cite_inputs_path = \"/workspace/tripx/MCS/big_data/data/sub_sample_train_cite_inputs.csv\"\n",
    "# train_cite_inputs_data = my_spark.read.csv(train_cite_inputs_path,  header=True, maxColumns=30000)\n",
    "# # train_cite_inputs_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cite_target_path = \"/workspace/tripx/MCS/big_data/data/train_cite_targets.csv\"\n",
    "train_cite_target_data = my_spark.read.csv(train_cite_target_path,  header=True)\n",
    "# train_cite_target_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cite_inputs_path = \"/workspace/tripx/MCS/big_data/data/sub_sample_test_cite_inputs.csv\"\n",
    "# test_cite_inputs_data = my_spark.read.csv(test_cite_inputs_path,  header=True,  maxColumns=30000)\n",
    "# # test_cite_inputs_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cite_target_data.createOrReplaceTempView('train_cite_target_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = train_cite_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols=(column.replace('.', '_') for column in train_output.columns)\n",
    "train_output = train_output.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 12/140 [00:00<00:01, 118.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 72.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in tqdm(train_output.columns[1:]):\n",
    "    # print(col_name)\n",
    "    train_output = train_output.withColumn(col_name,  col(col_name).cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extraced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        x_row = (x_row - np.mean(x_row)) / np.std(x_row)\n",
    "        x_zscore.append(x_row)\n",
    "    x_std = np.array(x_zscore)    \n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path='/dataset/NeurIPS2022/2nd-solution/kaggle/src_top2/senkin13/features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather(feature_path+'train_cite_inputs_id.feather')\n",
    "test_df = pd.read_feather(feature_path+'test_cite_inputs_id.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cite_inputs_svd_clr = np.load(feature_path+'cite_inputs_svd_clr_200.npy')\n",
    "train_cite_svd_clr = cite_inputs_svd_clr[:len(train_df)]\n",
    "test_cite_svd_clr = cite_inputs_svd_clr[len(train_df):]\n",
    "train_cite_svd_clr = zscore(train_cite_svd_clr)\n",
    "test_cite_svd_clr = zscore(test_cite_svd_clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70988, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cite_svd_clr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48203, 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cite_svd_clr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_df = map(lambda x: (int(1), Vectors.dense(x)), train_cite_svd_clr)\n",
    "train_feature_df = my_spark.createDataFrame(train_feature_df,schema=[\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_feature_df.join(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, cell_id: string, CD86: float, CD274: float, CD270: float, CD155: float, CD112: float, CD47: float, CD48: float, CD40: float, CD154: float, CD52: float, CD3: float, CD8: float, CD56: float, CD19: float, CD33: float, CD11c: float, HLA-A-B-C: float, CD45RA: float, CD123: float, CD7: float, CD105: float, CD49f: float, CD194: float, CD4: float, CD44: float, CD14: float, CD16: float, CD25: float, CD45RO: float, CD279: float, TIGIT: float, Mouse-IgG1: float, Mouse-IgG2a: float, Mouse-IgG2b: float, Rat-IgG2b: float, CD20: float, CD335: float, CD31: float, Podoplanin: float, CD146: float, IgM: float, CD5: float, CD195: float, CD32: float, CD196: float, CD185: float, CD103: float, CD69: float, CD62L: float, CD161: float, CD152: float, CD223: float, KLRG1: float, CD27: float, CD107a: float, CD95: float, CD134: float, HLA-DR: float, CD1c: float, CD11b: float, CD64: float, CD141: float, CD1d: float, CD314: float, CD35: float, CD57: float, CD272: float, CD278: float, CD58: float, CD39: float, CX3CR1: float, CD24: float, CD21: float, CD11a: float, CD79b: float, CD244: float, CD169: float, integrinB7: float, CD268: float, CD42b: float, CD54: float, CD62P: float, CD119: float, TCR: float, Rat-IgG1: float, Rat-IgG2a: float, CD192: float, CD122: float, FceRIa: float, CD41: float, CD137: float, CD163: float, CD83: float, CD124: float, CD13: float, CD2: float, CD226: float, CD29: float, CD303: float, CD49b: float, CD81: float, IgD: float, CD18: float, CD28: float, CD38: float, CD127: float, CD45: float, CD22: float, CD71: float, CD26: float, CD115: float, CD63: float, CD304: float, CD36: float, CD172a: float, CD72: float, CD158: float, CD93: float, CD49a: float, CD49d: float, CD73: float, CD9: float, TCRVa7_2: float, TCRVd2: float, LOX-1: float, CD158b: float, CD158e1: float, CD142: float, CD319: float, CD352: float, CD94: float, CD162: float, CD85j: float, CD23: float, CD328: float, HLA-E: float, CD82: float, CD101: float, CD88: float, CD224: float]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = train_output.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr_model, train_data, column_name):\n",
    "    data_vector = train_data.select(\"features\", column_name)\n",
    "    data_vector = data_vector.withColumnRenamed(column_name, \"label\")\n",
    "    # data_vector.show(5)\n",
    "    # input()\n",
    "    lr_model = lr.fit(data_vector)\n",
    "    print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "    print(\"Intercept: \" + str(lr_model.intercept))\n",
    "    trainingSummary = lr_model.summary\n",
    "    print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "    print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 01:05:26 WARN TaskSetManager: Stage 16 contains a task of very large size (1314 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/07/27 01:05:26 WARN TaskSetManager: Stage 18 contains a task of very large size (1314 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/07/27 01:05:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/07/27 01:05:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "[Stage 18:>                                                       (0 + 96) / 96]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m output_columns:\n\u001b[0;32m----> 2\u001b[0m     train(lr, train_data, column_name)\n",
      "Cell \u001b[0;32mIn[65], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lr_model, train_data, column_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m data_vector \u001b[39m=\u001b[39m data_vector\u001b[39m.\u001b[39mwithColumnRenamed(column_name, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# data_vector.show(5)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# input()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m lr_model \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(data_vector)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCoefficients: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lr_model\u001b[39m.\u001b[39mcoefficients))\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIntercept: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lr_model\u001b[39m.\u001b[39mintercept))\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    336\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1034\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError while sending\u001b[39m\u001b[39m\"\u001b[39m, e, proto\u001b[39m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[39mif\u001b[39;00m answer\u001b[39m.\u001b[39mstartswith(proto\u001b[39m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                       (0 + 96) / 96]\r"
     ]
    }
   ],
   "source": [
    "for column_name in output_columns:\n",
    "    train(lr, train_data, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_df = map(lambda x: (int(1), Vectors.dense(x)), test_cite_svd_clr)\n",
    "test_feature_df = my_spark.createDataFrame(test_feature_df,schema=[\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_feature_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
